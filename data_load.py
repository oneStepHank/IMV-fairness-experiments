import os
import sys
import json
import pandas as pd
import numpy as np
from pathlib import Path
from tqdm import tqdm
from sklearn.model_selection import train_test_split
import config

# --- ì„¤ì • ---
MIMIC_DIR = config.MIMIC_DIR
DATA_PATH = config.DATA_PATH
CHUNK_SIZE = config.CHUNK_SIZE

print("--- 0. ì´ˆê¸° ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ ---")

# --- í—¬í¼ í•¨ìˆ˜ ---
def get_mimic_col(df, target_name):
    for col in df.columns:
        if col.upper() == target_name.upper():
            return col
    return None

# ==============================================================================
# 1. ì´ˆê¸° ë°ì´í„° ë¡œë“œ
# ==============================================================================
print("\n--- 1. ì´ˆê¸° ë°ì´í„° ë¡œë“œ ---")
print("â³ í•„ìˆ˜ MIMIC-III í…Œì´ë¸” ë¡œë“œ ì¤‘...")
df_a = pd.read_csv(MIMIC_DIR / 'ADMISSIONS.csv')
df_p = pd.read_csv(MIMIC_DIR / 'PATIENTS.csv')
df_i = pd.read_csv(MIMIC_DIR / 'ICUSTAYS.csv')
print("âœ… í•„ìˆ˜ í…Œì´ë¸” ë¡œë“œ ì™„ë£Œ: ADMISSIONS, PATIENTS, ICUSTAYS")

# ì§ˆë³‘ ì¤‘ì¦ë„ ì ìˆ˜ ë¡œë“œ
print("â³ ì§ˆë³‘ ì¤‘ì¦ë„ ì ìˆ˜ íŒŒì¼ ë¡œë“œ ì¤‘...")
score_files = {
    'OASIS': Path('./data/oasis.csv'),
    'SAPSII': Path('./data/sapsii.csv'),
    'SOFA': Path('./data/sofa.csv')
}
df_scores = {}
for score_name, score_path in score_files.items():
    if score_path.exists():
        df_scores[score_name] = pd.read_csv(score_path, low_memory=False)
        print(f"   âœ… {score_name} ë¡œë“œ ì™„ë£Œ: {len(df_scores[score_name])} rows")
    else:
        print(f"   âš ï¸ {score_name} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {score_path}")
        df_scores[score_name] = None

# ==============================================================================
# 2. ì½”í˜¸íŠ¸ ì„ ì • ë° ë°ì´í„° ì¶”ì¶œ (í†µí•© í”„ë¡œì„¸ìŠ¤)
# ==============================================================================
print("\n" + "="*80)
print("ì½”í˜¸íŠ¸ ì„ ì • ë° ë°ì´í„° ì¶”ì¶œ (í†µí•© í”„ë¡œì„¸ìŠ¤)")
print("="*80)

# 2.1. ì„±ì¸ í™˜ì í•„í„°ë§
print("â³ í™˜ì ë° ì…ì› ì •ë³´ ì²˜ë¦¬ ì¤‘...")
dob_col = get_mimic_col(df_p, 'DOB')
gender_col = get_mimic_col(df_p, 'GENDER')
df_p_select = df_p[['SUBJECT_ID', dob_col, gender_col]].copy()
df_p_select.rename(columns={dob_col: 'DOB', gender_col: 'GENDER'}, inplace=True)

df_i = pd.merge(df_i, df_p_select, on='SUBJECT_ID', how='left')
df_i['DOB'] = pd.to_datetime(df_i['DOB'], errors='coerce')
df_i['INTIME'] = pd.to_datetime(df_i['INTIME'], errors='coerce')
df_i.dropna(subset=['DOB', 'INTIME'], inplace=True)

df_i['ADMIT_YEAR'] = df_i['INTIME'].dt.year
df_i['BIRTH_YEAR'] = df_i['DOB'].dt.year
df_i['AGE'] = df_i['ADMIT_YEAR'] - df_i['BIRTH_YEAR']

# ì„±ì¸ í™˜ì (18ì„¸ ì´ìƒ 90ì„¸ ë¯¸ë§Œ)
# Note: MIMIC-IIIì—ì„œ 90ì„¸ ì´ìƒì€ í”„ë¼ì´ë²„ì‹œë¥¼ ìœ„í•´ 300ì„¸ ì´ìƒìœ¼ë¡œ í‘œì‹œë˜ë¯€ë¡œ
# ì‹¤ì œ ë‚˜ì´ ê³„ì‚° ì‹œ ë§¤ìš° í° ê°’ì´ ë‚˜ì˜¬ ìˆ˜ ìˆìŒ. ì´ë¥¼ í•„í„°ë§ìœ¼ë¡œ ì œì™¸.
df_cohort = df_i[(df_i['AGE'] >= 18) & (df_i['AGE'] < 90)].copy()
n_adult = df_cohort['SUBJECT_ID'].nunique()

print(f"\nğŸ“Š Step 1: Adult Patients (18 years old and < 90 years old)")
print(f"   Our cohort:   {n_adult:,} patients")
print(f"   Paper:        38,597 patients")

# 2.2. MV í™˜ì ì‹ë³„ (ICD-9 + CHARTEVENTS í†µí•©)
print("\nâ³ MV í™˜ì ì‹ë³„ ë° í™œë ¥ì§•í›„ ì¶”ì¶œ (Single Pass)...")

# (1) ICD-9 ê¸°ë°˜ MV í™˜ì ì‹ë³„
df_proc = pd.read_csv(MIMIC_DIR / 'PROCEDURES_ICD.csv', low_memory=False)
proc_icd_col = get_mimic_col(df_proc, 'ICD9_CODE')
mv_codes = ['9670', '9671', '9672']
mv_hadm_ids_icd9 = set(df_proc[df_proc[proc_icd_col].astype(str).isin(mv_codes)]['HADM_ID'].unique())
print(f"   - ICD-9 ì½”ë“œë¡œ ì‹ë³„ëœ MV í™˜ì(HADM_ID): {len(mv_hadm_ids_icd9):,}ëª…")

# (2) CHARTEVENTS ê¸°ë°˜ MV ì‹ë³„ ë° í™œë ¥ì§•í›„ ì¶”ì¶œ
# SQL Logic Implementation
# MechVent ITEMIDs from the provided SQL
MV_ITEMIDS_SQL = [
    720, 223849, 223848, 223849, 467, # Settings with value checks
    445, 448, 449, 450, 1340, 1486, 1600, 224687, # Minute volume
    639, 654, 681, 682, 683, 684, 224685, 224684, 224686, # Tidal volume
    218, 436, 535, 444, 459, 224697, 224695, 224696, 224746, 224747, # RespPressure
    221, 1, 1211, 1655, 2000, 226873, 224738, 224419, 224750, 227187, # Insp pressure
    543, # PlateauPressure
    5865, 5866, 224707, 224709, 224705, 224706, # APRV pressure
    60, 437, 505, 506, 686, 220339, 224700, # PEEP
    3459, # High pressure relief
    501, 502, 503, 224702, # PCV
    223, 667, 668, 669, 670, 671, 672, # TCPCV
    224701 # PSVlevel
]
MV_ITEMIDS_SET = set(MV_ITEMIDS_SQL)

# í™œë ¥ì§•í›„ ITEMID
VITAL_SIGN_MAP = {
    'HR': [211, 220045], 
    'SBP': [51, 455, 220179, 220050], 
    'DBP': [8368, 8440, 220180, 220051],
    'MAP': [52, 456, 220181, 220052], 
    'Temp': [678, 679, 223761, 223762]
}
VITAL_SIGN_ITEMIDS = [item for sublist in VITAL_SIGN_MAP.values() for item in sublist]
VITAL_SIGN_ITEMIDS_SET = set(VITAL_SIGN_ITEMIDS)

# í†µí•© ITEMID ì„¸íŠ¸ (í•„í„°ë§ìš©)
ALL_TARGET_ITEMIDS = MV_ITEMIDS_SET | VITAL_SIGN_ITEMIDS_SET

# ë°ì´í„° ìˆ˜ì§‘
mv_icustay_ids_chart = set()
ce_24hr_list = []

chartevents_path = MIMIC_DIR / 'CHARTEVENTS.csv'
file_size = os.path.getsize(chartevents_path)
estimated_chunks = max(1, file_size // (CHUNK_SIZE * 100))

reader_ce = pd.read_csv(
    chartevents_path, 
    chunksize=CHUNK_SIZE, 
    low_memory=False, 
    iterator=True
)

# ì½”í˜¸íŠ¸ì˜ ICUSTAY_ID ëª©ë¡ (ì„±ì¸ í™˜ì ì „ì²´ ëŒ€ìƒ)
adult_icustay_ids = set(df_cohort['ICUSTAY_ID'].unique())

print(f"â³ Reading CHARTEVENTS.csv (Raw)... This may take 10-15 minutes.")
for chunk in tqdm(reader_ce, desc="Processing CHARTEVENTS", total=estimated_chunks):
    chunk.columns = [c.upper() for c in chunk.columns]
    
    # 1. ì„±ì¸ í™˜ì í•„í„°ë§
    chunk = chunk[chunk['ICUSTAY_ID'].isin(adult_icustay_ids)]
    if chunk.empty: continue
    
    # 2. ê´€ì‹¬ ITEMID í•„í„°ë§
    chunk = chunk[chunk['ITEMID'].isin(ALL_TARGET_ITEMIDS)]
    if chunk.empty: continue
    
    # 3. MV í™˜ì ì‹ë³„ (SQL Logic ì ìš©)
    # SQL Logic:
    # when itemid = 720 and value != 'Other/Remarks' THEN 1
    # when itemid = 223848 and value != 'Other' THEN 1
    # when itemid = 223849 then 1
    # when itemid = 467 and value = 'Ventilator' THEN 1
    # else (other itemids) THEN 1
    
    mv_chunk = chunk[chunk['ITEMID'].isin(MV_ITEMIDS_SET)].copy()
    if not mv_chunk.empty:
        if 'VALUE' in mv_chunk.columns:
            mv_chunk['VALUE'] = mv_chunk['VALUE'].astype(str)
            
            # ì¡°ê±´ë³„ ë§ˆìŠ¤í¬ ìƒì„±
            mask_720 = (mv_chunk['ITEMID'] == 720) & (mv_chunk['VALUE'] != 'Other/Remarks')
            mask_223848 = (mv_chunk['ITEMID'] == 223848) & (mv_chunk['VALUE'] != 'Other')
            mask_467 = (mv_chunk['ITEMID'] == 467) & (mv_chunk['VALUE'] == 'Ventilator')
            
            # ë‚˜ë¨¸ì§€ ITEMIDëŠ” ì¡´ì¬í•˜ê¸°ë§Œ í•˜ë©´ MVë¡œ ê°„ì£¼
            other_mv_itemids = MV_ITEMIDS_SET - {720, 223848, 467}
            mask_others = mv_chunk['ITEMID'].isin(other_mv_itemids)
            
            # ìµœì¢… MV ë§ˆìŠ¤í¬
            final_mv_mask = mask_720 | mask_223848 | mask_467 | mask_others
            
            valid_mv = mv_chunk[final_mv_mask]
            mv_icustay_ids_chart.update(valid_mv['ICUSTAY_ID'].unique())
        else:
            # VALUE ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ITEMIDë§Œìœ¼ë¡œ íŒë‹¨ (ì˜ˆì™¸ì  ìƒí™©)
            mv_icustay_ids_chart.update(mv_chunk['ICUSTAY_ID'].unique())
            
    # 4. í™œë ¥ì§•í›„ ë°ì´í„° ìˆ˜ì§‘ (ì¼ë‹¨ ì €ì¥, ë‚˜ì¤‘ì— MV í™˜ìë§Œ í•„í„°ë§)
    vital_chunk = chunk[chunk['ITEMID'].isin(VITAL_SIGN_ITEMIDS_SET)]
    if not vital_chunk.empty:
        ce_24hr_list.append(vital_chunk[['ICUSTAY_ID', 'ITEMID', 'CHARTTIME', 'VALUENUM']])

print(f"   - CHARTEVENTSë¡œ ì‹ë³„ëœ MV í™˜ì(ICUSTAY_ID): {len(mv_icustay_ids_chart):,}ëª…")

# (3) MV í™˜ì í†µí•© (ICD-9 OR CHARTEVENTS)
# ICD-9 HADM_ID -> ICUSTAY_ID ë³€í™˜
mv_icustay_ids_icd9 = set(df_cohort[df_cohort['HADM_ID'].isin(mv_hadm_ids_icd9)]['ICUSTAY_ID'].unique())
final_mv_icustay_ids = mv_icustay_ids_icd9 | mv_icustay_ids_chart

# ì½”í˜¸íŠ¸ í•„í„°ë§
df_mv_cohort = df_cohort[df_cohort['ICUSTAY_ID'].isin(final_mv_icustay_ids)].copy()
n_mv_combined = df_mv_cohort['SUBJECT_ID'].nunique()

print(f"\nğŸ“Š Step 2: Mechanically Ventilated Patients (Combined Filter)")
print(f"   Our cohort:   {n_mv_combined:,} patients")
print(f"   Paper:        28,530 patients")

# (4) í™œë ¥ì§•í›„ ë°ì´í„° ì •ë¦¬ (MV í™˜ì & 24ì‹œê°„ ì´ë‚´)
print("\nâ³ í™œë ¥ì§•í›„ ë°ì´í„° ì •ë¦¬ ì¤‘...")
if ce_24hr_list:
    df_ce_all = pd.concat(ce_24hr_list)
    
    # MV í™˜ìë§Œ ë‚¨ê¸°ê¸°
    df_ce_all = df_ce_all[df_ce_all['ICUSTAY_ID'].isin(final_mv_icustay_ids)]
    
    # ì‹œê°„ í•„í„°ë§
    cohort_times = df_mv_cohort[['ICUSTAY_ID', 'INTIME']].drop_duplicates()
    df_ce_all['CHARTTIME'] = pd.to_datetime(df_ce_all['CHARTTIME'], errors='coerce')
    df_ce_all = pd.merge(df_ce_all, cohort_times, on='ICUSTAY_ID', how='inner')
    
    df_ce_24hr = df_ce_all[
        (df_ce_all['CHARTTIME'] >= df_ce_all['INTIME']) & 
        (df_ce_all['CHARTTIME'] <= df_ce_all['INTIME'] + pd.Timedelta(hours=24))
    ][['ICUSTAY_ID', 'ITEMID', 'VALUENUM']]
    
    print(f"   âœ… í™œë ¥ì§•í›„ ë°ì´í„° ì •ë¦¬ ì™„ë£Œ: {len(df_ce_24hr):,} rows")
else:
    df_ce_24hr = pd.DataFrame(columns=['ICUSTAY_ID', 'ITEMID', 'VALUENUM'])
    print("   âš ï¸ í™œë ¥ì§•í›„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

# 2.3. First ICU Stay í•„í„°ë§
df_mv_cohort.sort_values(by=['SUBJECT_ID', 'INTIME'], inplace=True)
df_mv_cohort = df_mv_cohort.drop_duplicates(subset=['SUBJECT_ID'], keep='first').copy()
n_first_icu = df_mv_cohort['SUBJECT_ID'].nunique()

print(f"\nğŸ“Š Step 3: First ICU Stay Only")
print(f"   Our cohort:   {n_first_icu:,} patients")
print(f"   Paper:        28,530 patients (selection criteria met)")
print(f"   Difference:   {n_first_icu - 28530:+,} ({(n_first_icu/28530-1)*100:+.1f}%)")

# Step 4: LOS < 24ì‹œê°„ ì œì™¸
if 'LOS' in df_mv_cohort.columns:
    n_before_los = df_mv_cohort['SUBJECT_ID'].nunique()
    df_mv_cohort = df_mv_cohort[df_mv_cohort['LOS'] >= 1.0].copy()
    n_after_los = df_mv_cohort['SUBJECT_ID'].nunique()
    print(f"\nğŸ“Š Step 4: LOS â‰¥ 24 hours")
    print(f"   Excluded:     {n_before_los - n_after_los:,} patients")
    print(f"   Remaining:    {n_after_los:,} patients")

# Step 5: ë³‘ì› ì‚¬ë§ë¥  ì •ë³´ ì¶”ê°€ ë° ìµœì¢… ì½”í˜¸íŠ¸
hosp_expire_col = get_mimic_col(df_a, 'HOSPITAL_EXPIRE_FLAG')
cols_to_merge = ['HADM_ID', hosp_expire_col, 'INSURANCE', 'ETHNICITY']
df_mv_cohort = pd.merge(df_mv_cohort, df_a[cols_to_merge], on='HADM_ID', how='left')
df_mv_cohort['hospital_mortality'] = df_mv_cohort[hosp_expire_col]

n_before_mortality = df_mv_cohort['SUBJECT_ID'].nunique()
df_mv_cohort.dropna(subset=['hospital_mortality'], inplace=True)
n_final = df_mv_cohort['SUBJECT_ID'].nunique()

print(f"\nğŸ“Š Step 5: Final Cohort (after logic check)")
print(f"   Excluded:     {n_before_mortality - n_final:,} patients (missing mortality data)")
print(f"   Our cohort:   {n_final:,} patients")
print(f"   Paper:        25,659 patients")
print(f"   Difference:   {n_final - 25659:+,} ({(n_final/25659-1)*100:+.1f}%)")

# ì‚¬ë§ë¥  í†µê³„
n_survivors = (df_mv_cohort['hospital_mortality'] == 0).sum()
n_deaths = (df_mv_cohort['hospital_mortality'] == 1).sum()
mortality_rate = n_deaths / n_final * 100

print(f"\nğŸ“Š Mortality Statistics")
print(f"   Survivors:    {n_survivors:,} ({n_survivors/n_final*100:.1f}%)")
print(f"   Deaths:       {n_deaths:,} ({mortality_rate:.1f}%)")
print(f"   Paper:        13,987 survivors (54.5%), 11,672 deaths (45.5%)")

print("="*80)

# ETHNICITY ë§¤í•‘
print("\n-> ETHNICITY ë§¤í•‘ ì¤‘...")
import json

ethnicity_map_file = Path('ethnicity.json')
if ethnicity_map_file.exists():
    with open(ethnicity_map_file, 'r', encoding='utf-8') as f:
        ethnicity_config = json.load(f)
    
    ethnicity_mapping = {}
    for mapping in ethnicity_config['mappings']:
        main_code = mapping['code']
        for detail_code in mapping['lists']:
            ethnicity_mapping[detail_code.upper()] = main_code
    
    df_mv_cohort['ETHNICITY'] = df_mv_cohort['ETHNICITY'].str.upper().map(ethnicity_mapping).fillna('OTHER')
    
    print(f"   âœ… ETHNICITY ë§¤í•‘ ì™„ë£Œ")
    print(f"   ë§¤í•‘ëœ ê·¸ë£¹: {df_mv_cohort['ETHNICITY'].value_counts().to_dict()}")
else:
    print(f"   âš ï¸ ethnicity.json íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë§¤í•‘ì„ ê±´ë„ˆëœë‹ˆë‹¤.")


# 2.2. MV í™˜ì ì‹ë³„ (ICD-9 + CHARTEVENTS í†µí•©)
print("\nâ³ MV í™˜ì ì‹ë³„ ë° í™œë ¥ì§•í›„ ì¶”ì¶œ (Single Pass)...")

# (1) ICD-9 ê¸°ë°˜ MV í™˜ì ì‹ë³„
df_proc = pd.read_csv(MIMIC_DIR / 'PROCEDURES_ICD.csv', low_memory=False)
proc_icd_col = get_mimic_col(df_proc, 'ICD9_CODE')
mv_codes = ['9670', '9671', '9672']
mv_hadm_ids_icd9 = set(df_proc[df_proc[proc_icd_col].astype(str).isin(mv_codes)]['HADM_ID'].unique())
print(f"   - ICD-9 ì½”ë“œë¡œ ì‹ë³„ëœ MV í™˜ì(HADM_ID): {len(mv_hadm_ids_icd9):,}ëª…")

# (2) CHARTEVENTS ê¸°ë°˜ MV ì‹ë³„ ë° í™œë ¥ì§•í›„ ì¶”ì¶œ
# MV ê´€ë ¨ ITEMID (SQL ë¡œì§ ê¸°ë°˜)
MV_ITEMIDS = [
    720, 223849, 223848, 445, 448, 449, 450, 1340, 1486, 1600, 224687,
    639, 654, 681, 682, 683, 684, 224685, 224684, 224686,
    218, 436, 535, 444, 459, 224697, 224695, 224696, 224746, 224747,
    221, 1, 1211, 1655, 2000, 226873, 224738, 224419, 224750, 227187,
    543, 5865, 5866, 224707, 224709, 224705, 224706,
    60, 437, 505, 506, 686, 220339, 224700, 3459, 501, 502, 503, 224702,
    223, 667, 668, 669, 670, 671, 672, 224701
]
MV_ITEMIDS_SET = set(MV_ITEMIDS)

# í™œë ¥ì§•í›„ ITEMID
VITAL_SIGN_MAP = {
    'HR': [211, 220045], 
    'SBP': [51, 455, 220179, 220050], 
    'DBP': [8368, 8440, 220180, 220051],
    'MAP': [52, 456, 220181, 220052], 
    'Temp': [678, 679, 223761, 223762]
}
VITAL_SIGN_ITEMIDS = [item for sublist in VITAL_SIGN_MAP.values() for item in sublist]
VITAL_SIGN_ITEMIDS_SET = set(VITAL_SIGN_ITEMIDS)

# í†µí•© ITEMID ì„¸íŠ¸ (í•„í„°ë§ìš©)
ALL_TARGET_ITEMIDS = MV_ITEMIDS_SET | VITAL_SIGN_ITEMIDS_SET

# ë°ì´í„° ìˆ˜ì§‘
mv_icustay_ids_chart = set()
ce_24hr_list = []

chartevents_path = MIMIC_DIR / 'CHARTEVENTS.csv'
file_size = os.path.getsize(chartevents_path)
estimated_chunks = max(1, file_size // (CHUNK_SIZE * 100))

reader_ce = pd.read_csv(
    chartevents_path, 
    chunksize=CHUNK_SIZE, 
    low_memory=False, 
    iterator=True
)

# ì½”í˜¸íŠ¸ì˜ ICUSTAY_ID ëª©ë¡ (ì„±ì¸ í™˜ì ì „ì²´ ëŒ€ìƒ)
adult_icustay_ids = set(df_cohort['ICUSTAY_ID'].unique())

print(f"â³ Reading CHARTEVENTS.csv (Raw)... This may take 10-15 minutes.")
for chunk in tqdm(reader_ce, desc="Processing CHARTEVENTS", total=estimated_chunks):
    chunk.columns = [c.upper() for c in chunk.columns]
    
    # 1. ì„±ì¸ í™˜ì í•„í„°ë§
    chunk = chunk[chunk['ICUSTAY_ID'].isin(adult_icustay_ids)]
    if chunk.empty: continue
    
    # 2. ê´€ì‹¬ ITEMID í•„í„°ë§
    chunk = chunk[chunk['ITEMID'].isin(ALL_TARGET_ITEMIDS)]
    if chunk.empty: continue
    
    # 3. MV í™˜ì ì‹ë³„ (CHARTEVENTS ê¸°ë°˜)
    mv_chunk = chunk[chunk['ITEMID'].isin(MV_ITEMIDS_SET)]
    if not mv_chunk.empty:
        # VALUE í•„í„°ë§ (ê°„ì†Œí™”: 'Other' ë“± ì œì™¸)
        if 'VALUE' in mv_chunk.columns:
            val_str = mv_chunk['VALUE'].astype(str).str.lower()
            # ITEMID 720, 223848ì˜ 'other' ì œì™¸
            exclude_mask = (mv_chunk['ITEMID'].isin([720, 223848])) & (val_str.str.contains('other', na=False))
            
            valid_mv = mv_chunk[~exclude_mask]
            mv_icustay_ids_chart.update(valid_mv['ICUSTAY_ID'].unique())
        else:
            mv_icustay_ids_chart.update(mv_chunk['ICUSTAY_ID'].unique())
            
    # 4. í™œë ¥ì§•í›„ ë°ì´í„° ìˆ˜ì§‘ (ì¼ë‹¨ ì €ì¥, ë‚˜ì¤‘ì— MV í™˜ìë§Œ í•„í„°ë§)
    vital_chunk = chunk[chunk['ITEMID'].isin(VITAL_SIGN_ITEMIDS_SET)]
    if not vital_chunk.empty:
        ce_24hr_list.append(vital_chunk[['ICUSTAY_ID', 'ITEMID', 'CHARTTIME', 'VALUENUM']])

print(f"   - CHARTEVENTSë¡œ ì‹ë³„ëœ MV í™˜ì(ICUSTAY_ID): {len(mv_icustay_ids_chart):,}ëª…")

# (3) MV í™˜ì í†µí•© (ICD-9 OR CHARTEVENTS)
# ICD-9 HADM_ID -> ICUSTAY_ID ë³€í™˜
mv_icustay_ids_icd9 = set(df_cohort[df_cohort['HADM_ID'].isin(mv_hadm_ids_icd9)]['ICUSTAY_ID'].unique())
final_mv_icustay_ids = mv_icustay_ids_icd9 | mv_icustay_ids_chart

# ì½”í˜¸íŠ¸ í•„í„°ë§
df_mv_cohort = df_cohort[df_cohort['ICUSTAY_ID'].isin(final_mv_icustay_ids)].copy()
n_mv_combined = df_mv_cohort['SUBJECT_ID'].nunique()

print(f"\nğŸ“Š Step 2: Mechanically Ventilated Patients (Combined Filter)")
print(f"   Our cohort:   {n_mv_combined:,} patients")
print(f"   Paper:        28,530 patients")

# (4) í™œë ¥ì§•í›„ ë°ì´í„° ì •ë¦¬ (MV í™˜ì & 24ì‹œê°„ ì´ë‚´)
print("\nâ³ í™œë ¥ì§•í›„ ë°ì´í„° ì •ë¦¬ ì¤‘...")
if ce_24hr_list:
    df_ce_all = pd.concat(ce_24hr_list)
    
    # MV í™˜ìë§Œ ë‚¨ê¸°ê¸°
    df_ce_all = df_ce_all[df_ce_all['ICUSTAY_ID'].isin(final_mv_icustay_ids)]
    
    # ì‹œê°„ í•„í„°ë§
    cohort_times = df_mv_cohort[['ICUSTAY_ID', 'INTIME']].drop_duplicates()
    df_ce_all['CHARTTIME'] = pd.to_datetime(df_ce_all['CHARTTIME'], errors='coerce')
    df_ce_all = pd.merge(df_ce_all, cohort_times, on='ICUSTAY_ID', how='inner')
    
    df_ce_24hr = df_ce_all[
        (df_ce_all['CHARTTIME'] >= df_ce_all['INTIME']) & 
        (df_ce_all['CHARTTIME'] <= df_ce_all['INTIME'] + pd.Timedelta(hours=24))
    ][['ICUSTAY_ID', 'ITEMID', 'VALUENUM']]
    
    print(f"   âœ… í™œë ¥ì§•í›„ ë°ì´í„° ì •ë¦¬ ì™„ë£Œ: {len(df_ce_24hr):,} rows")
else:
    df_ce_24hr = pd.DataFrame(columns=['ICUSTAY_ID', 'ITEMID', 'VALUENUM'])
    print("   âš ï¸ í™œë ¥ì§•í›„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

# 2.3. First ICU Stay í•„í„°ë§
df_mv_cohort.sort_values(by=['SUBJECT_ID', 'INTIME'], inplace=True)
df_mv_cohort = df_mv_cohort.drop_duplicates(subset=['SUBJECT_ID'], keep='first').copy()
n_first_icu = df_mv_cohort['SUBJECT_ID'].nunique()

print(f"\nğŸ“Š Step 3: First ICU Stay Only")
print(f"   Our cohort:   {n_first_icu:,} patients")
print(f"   Paper:        28,530 patients (selection criteria met)")
print(f"   Difference:   {n_first_icu - 28530:+,} ({(n_first_icu/28530-1)*100:+.1f}%)")

# Step 4: LOS < 24ì‹œê°„ ì œì™¸
if 'LOS' in df_mv_cohort.columns:
    n_before_los = df_mv_cohort['SUBJECT_ID'].nunique()
    df_mv_cohort = df_mv_cohort[df_mv_cohort['LOS'] >= 1.0].copy()
    n_after_los = df_mv_cohort['SUBJECT_ID'].nunique()
    print(f"\nğŸ“Š Step 4: LOS â‰¥ 24 hours")
    print(f"   Excluded:     {n_before_los - n_after_los:,} patients")
    print(f"   Remaining:    {n_after_los:,} patients")

# Step 5: ë³‘ì› ì‚¬ë§ë¥  ì •ë³´ ì¶”ê°€ ë° ìµœì¢… ì½”í˜¸íŠ¸
hosp_expire_col = get_mimic_col(df_a, 'HOSPITAL_EXPIRE_FLAG')
cols_to_merge = ['HADM_ID', hosp_expire_col, 'INSURANCE', 'ETHNICITY']
df_mv_cohort = pd.merge(df_mv_cohort, df_a[cols_to_merge], on='HADM_ID', how='left')
df_mv_cohort['hospital_mortality'] = df_mv_cohort[hosp_expire_col]

n_before_mortality = df_mv_cohort['SUBJECT_ID'].nunique()
df_mv_cohort.dropna(subset=['hospital_mortality'], inplace=True)
n_final = df_mv_cohort['SUBJECT_ID'].nunique()

print(f"\nğŸ“Š Step 5: Final Cohort (after logic check)")
print(f"   Excluded:     {n_before_mortality - n_final:,} patients (missing mortality data)")
print(f"   Our cohort:   {n_final:,} patients")
print(f"   Paper:        25,659 patients")
print(f"   Difference:   {n_final - 25659:+,} ({(n_final/25659-1)*100:+.1f}%)")

# ì‚¬ë§ë¥  í†µê³„
n_survivors = (df_mv_cohort['hospital_mortality'] == 0).sum()
n_deaths = (df_mv_cohort['hospital_mortality'] == 1).sum()
mortality_rate = n_deaths / n_final * 100

print(f"\nğŸ“Š Mortality Statistics")
print(f"   Survivors:    {n_survivors:,} ({n_survivors/n_final*100:.1f}%)")
print(f"   Deaths:       {n_deaths:,} ({mortality_rate:.1f}%)")
print(f"   Paper:        13,987 survivors (54.5%), 11,672 deaths (45.5%)")

print("="*80)

# ETHNICITY ë§¤í•‘
print("\n-> ETHNICITY ë§¤í•‘ ì¤‘...")
import json

ethnicity_map_file = Path('ethnicity.json')
if ethnicity_map_file.exists():
    with open(ethnicity_map_file, 'r', encoding='utf-8') as f:
        ethnicity_config = json.load(f)
    
    ethnicity_mapping = {}
    for mapping in ethnicity_config['mappings']:
        main_code = mapping['code']
        for detail_code in mapping['lists']:
            ethnicity_mapping[detail_code.upper()] = main_code
    
    df_mv_cohort['ETHNICITY'] = df_mv_cohort['ETHNICITY'].str.upper().map(ethnicity_mapping).fillna('OTHER')
    
    print(f"   âœ… ETHNICITY ë§¤í•‘ ì™„ë£Œ")
    print(f"   ë§¤í•‘ëœ ê·¸ë£¹: {df_mv_cohort['ETHNICITY'].value_counts().to_dict()}")
else:
    print(f"   âš ï¸ ethnicity.json íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë§¤í•‘ì„ ê±´ë„ˆëœë‹ˆë‹¤.")


# ==============================================================================
# 3. íŠ¹ì§• ì¶”ì¶œ
# ==============================================================================
print("\n--- 3. íŠ¹ì§• ì¶”ì¶œ ë° ì •ë¦¬ ---")

# 3.1. DIAGNOSES_ICD ë¡œë“œ (dtype=str í•„ìˆ˜)
print("â³ DIAGNOSES_ICD.csv ë¡œë“œ ì¤‘...")
df_diag = pd.read_csv(MIMIC_DIR / 'DIAGNOSES_ICD.csv', dtype={'ICD9_CODE': str}, low_memory=False)

# 3.2. LABEVENTS ë¡œë“œ (Chunk ë‹¨ìœ„ ì²˜ë¦¬)
print("â³ LABEVENTS.csv ì²˜ë¦¬ ì¤‘ (Chunk ë‹¨ìœ„)...")
LAB_COLS = ['SUBJECT_ID', 'HADM_ID', 'ITEMID', 'CHARTTIME', 'VALUENUM']
LAB_ITEMIDS = {
    'lactate': [818, 1531, 225668], 
    'hgb': [50811, 51006, 51222, 51634, 52028], 
    'bun': [50882, 51006, 50931], 
    'creatinine': [50912], 
    'wbc': [51300, 51301], 
    'glucose': [50809, 50938, 51240], 
    'ph': [50820, 50931]
}
ALL_LAB_ITEMIDS = [item for sublist in LAB_ITEMIDS.values() for item in sublist]
ALL_LAB_ITEMIDS_SET = set(ALL_LAB_ITEMIDS)

lab_data_list = []
labevents_path = MIMIC_DIR / 'LABEVENTS.csv'
lab_file_size = os.path.getsize(labevents_path)
lab_estimated_chunks = max(1, lab_file_size // (CHUNK_SIZE * 100))

# ì½”í˜¸íŠ¸ì˜ HADM_ID ëª©ë¡
target_hadm_ids = set(df_mv_cohort['HADM_ID'].unique())

reader_lab = pd.read_csv(
    labevents_path, 
    usecols=LAB_COLS, 
    parse_dates=['CHARTTIME'], 
    chunksize=CHUNK_SIZE, 
    low_memory=False
)

for chunk in tqdm(reader_lab, desc="Processing LABEVENTS", total=lab_estimated_chunks):
    # 1. ì½”í˜¸íŠ¸ í™˜ì í•„í„°ë§ (HADM_ID ê¸°ì¤€)
    chunk = chunk[chunk['HADM_ID'].isin(target_hadm_ids)]
    if chunk.empty: continue
    
    # 2. ê´€ì‹¬ ITEMID í•„í„°ë§
    chunk = chunk[chunk['ITEMID'].isin(ALL_LAB_ITEMIDS_SET)]
    if not chunk.empty:
        lab_data_list.append(chunk)

if lab_data_list:
    df_lab_filtered = pd.concat(lab_data_list)
    print(f"   âœ… LABEVENTS ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df_lab_filtered):,} rows")
else:
    df_lab_filtered = pd.DataFrame(columns=LAB_COLS)
    print("   âš ï¸ LABEVENTS ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

# --- ICD-9 Comorbidity Mapping ---
# Based on: Quan et al. (2005) & MIMIC-Code
ICD9_MAP = {
    # Hypertension
    'Hypertension_uncomplicated': ['401'],
    'Hypertension_complicated': ['402', '403', '404', '405'],
    # Diabetes
    'Diabetes_uncomplicated': ['2500', '2501', '2502', '2503'],
    'Diabetes_complicated': ['2504', '2505', '2506', '2507', '2508', '2509'],
    # Others
    'Malignancy': [str(x) for x in range(140, 209)] + ['2386'],
    'Hematologic_disease': ['200', '201', '202', '203', '204', '205', '206', '207', '208'],
    'Metastasis': ['196', '197', '198', '199'],
    'Peripheral_vascular_disease': ['440', '441', '442', '443', '444', '447', '557', 'V434'],
    'Hypothyroidism': ['243', '244'],
    'Chronic_heart_failure': ['428'],
    'Stroke': ['430', '431', '432', '433', '434', '435', '436', '437', '438'],
    'Liver_disease': ['571', '570', '572'],
    # Angus Criteria
    'Sepsis': ['038', '99591', '99592', '78552'],
    'Respiratory_dysfunction': ['486', '51881', '51882', '51885', '78609'],
    'Cardiovascular_dysfunction': ['4580', '4588', '4589', '7855', '78551', '78559'],
    'Renal_dysfunction': ['580', '584', '585'],
    'Hepatic_dysfunction': ['570', '5722', '5733'],
    'Hematologic_dysfunction': ['2866', '2869', '2873', '2874', '2875'],
    'Metabolic_dysfunction': ['2762'],
    'Neurologic_dysfunction': ['293', '3481', '3483', '78001', '78009']
}

# 3.3. LABEVENTS 24ì‹œê°„ ë°ì´í„° í•„í„°ë§ ë° ì§‘ê³„
print("-> LABEVENTS 24ì‹œê°„ ë°ì´í„° í•„í„°ë§ ë° ì§‘ê³„...")
cohort_times_lab = df_mv_cohort[['HADM_ID', 'ICUSTAY_ID', 'INTIME']].drop_duplicates()
df_lab_merged = pd.merge(df_lab_filtered, cohort_times_lab, on='HADM_ID', how='inner')

valid_lab = df_lab_merged[
    (df_lab_merged['CHARTTIME'] >= df_lab_merged['INTIME']) & 
    (df_lab_merged['CHARTTIME'] <= df_lab_merged['INTIME'] + pd.Timedelta(hours=24))
]

# df_mv_cohort_final ì´ˆê¸°í™”
df_mv_cohort_final = df_mv_cohort.copy()

# Lab ì§‘ê³„ (Min, Max, Mean)
for lab_name, itemids in LAB_ITEMIDS.items():
    df_item = valid_lab[valid_lab['ITEMID'].isin(itemids)]
    
    # ê¸°ë³¸ í”„ë ˆì„ (ëª¨ë“  í™˜ì í¬í•¨)
    df_agg = pd.DataFrame({'ICUSTAY_ID': df_mv_cohort_final['ICUSTAY_ID'].unique()})
    
    if not df_item.empty:
        df_stats = df_item.groupby('ICUSTAY_ID')['VALUENUM'].agg(['min', 'max', 'mean']).reset_index()
        df_stats.columns = ['ICUSTAY_ID', f'min_{lab_name}', f'max_{lab_name}', f'mean_{lab_name}']
        df_agg = pd.merge(df_agg, df_stats, on='ICUSTAY_ID', how='left')
    else:
        print(f"   âš ï¸ {lab_name} ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. (NaNìœ¼ë¡œ ì±„ì›€)")
        for stat in ['min', 'max', 'mean']:
            df_agg[f'{stat}_{lab_name}'] = np.nan
            
    df_mv_cohort_final = pd.merge(df_mv_cohort_final, df_agg, on='ICUSTAY_ID', how='left')

# 3.4. ì§ˆë³‘ ì¤‘ì¦ë„ ì ìˆ˜ í†µí•©
print("-> ì§ˆë³‘ ì¤‘ì¦ë„ ì ìˆ˜ í†µí•© ì¤‘...")
for score_name, df_score in df_scores.items():
    if df_score is not None:
        score_col = score_name
        cols_to_merge = ['ICUSTAY_ID', score_col]
        
        if score_name == 'SOFA':
            sofa_subs = ['SOFA_Respiration', 'SOFA_Coagulation', 'SOFA_Liver', 
                         'SOFA_Cardiovascular', 'SOFA_CNS', 'SOFA_Renal']
            existing_subs = [col for col in df_score.columns if col in sofa_subs] # Check if sub-score columns exist
            cols_to_merge.extend(existing_subs)
            if existing_subs:
                print(f"      -> SOFA Sub-scores ë³‘í•©: {existing_subs}")
            else:
                print("      âš ï¸ SOFA Sub-scores ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. (SOFA_Respiration ë“±)")

        if score_col in df_score.columns:
            df_mv_cohort_final = pd.merge(df_mv_cohort_final, df_score[cols_to_merge], on='ICUSTAY_ID', how='left')
            print(f"   âœ… {score_name} ì ìˆ˜ ë³‘í•© ì™„ë£Œ (ê²°ì¸¡: {df_mv_cohort_final[score_col].isnull().sum()})")
        else:
            print(f"   âš ï¸ {score_name} íŒŒì¼ì— '{score_col}' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            df_mv_cohort_final[score_name] = np.nan
            if score_name == 'SOFA':
                for sub in sofa_subs: df_mv_cohort_final[sub] = np.nan
    else:
        print(f"   âš ï¸ {score_name} ë°ì´í„°ê°€ ì—†ì–´ NaNìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.")
        df_mv_cohort_final[score_name] = np.nan

# 3.5. ë™ë°˜ ì§ˆí™˜ (Comorbidities) ì¶”ê°€
print("-> ë™ë°˜ ì§ˆí™˜(Comorbidities) ë³€ìˆ˜ ì¶”ê°€ ì¤‘...")
diag_icd_col = get_mimic_col(df_diag, 'ICD9_CODE')
if diag_icd_col:
    # ë¬¸ìì—´ ë§¤ì¹­ì„ ìœ„í•´ ì»¬ëŸ¼ íƒ€ì… í™•ì¸
    df_diag[diag_icd_col] = df_diag[diag_icd_col].astype(str)
    
    for disease, codes in ICD9_MAP.items():
        # startswith ë§¤ì¹­ ì§€ì› (ì˜ˆ: '401'ì€ '4010', '4019' ë“±ì„ í¬í•¨í•´ì•¼ í•¨)
        # MIMIC ì½”ë“œëŠ” ì†Œìˆ˜ì ì´ ì—†ìŒ. Quan ì½”ë“œëŠ” 3~4ìë¦¬.
        # ì •í™•í•œ ë§¤ì¹­ + startswith ë§¤ì¹­ í˜¼ìš© í•„ìš”. ì—¬ê¸°ì„œëŠ” prefix ë§¤ì¹­ ì‚¬ìš©.
        
        # í•´ë‹¹ ì½”ë“œë¡œ ì‹œì‘í•˜ëŠ” ëª¨ë“  ì§„ë‹¨ ì½”ë“œ ì°¾ê¸°
        matched_codes = set()
        for code in codes:
            matched = df_diag[df_diag[diag_icd_col].str.startswith(code, na=False)][diag_icd_col].unique()
            matched_codes.update(matched)
        
        target_hadm_ids = df_diag[df_diag[diag_icd_col].isin(matched_codes)]['HADM_ID'].unique()
        df_mv_cohort_final[disease] = df_mv_cohort_final['HADM_ID'].isin(target_hadm_ids).astype(int)
        print(f"   - {disease}: {df_mv_cohort_final[disease].sum()}ëª…")
else:
    print("   âš ï¸ DIAGNOSES_ICD í…Œì´ë¸”ì—ì„œ ICD9_CODE ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# 3.6. í™œë ¥ì§•í›„ (Vital Signs) ì§‘ê³„
print("-> í™œë ¥ì§•í›„(Vital Signs) ì§‘ê³„ ì¤‘...")
if not df_ce_24hr.empty:
    for vital_name, itemids in VITAL_SIGN_MAP.items():
        df_item = df_ce_24hr[df_ce_24hr['ITEMID'].isin(itemids)]
        if not df_item.empty:
            df_agg = df_item.groupby('ICUSTAY_ID')['VALUENUM'].agg(['min', 'max', 'mean']).reset_index()
            df_agg.columns = ['ICUSTAY_ID', f'min_{vital_name.lower()}', f'max_{vital_name.lower()}', f'mean_{vital_name.lower()}']
            df_mv_cohort_final = pd.merge(df_mv_cohort_final, df_agg, on='ICUSTAY_ID', how='left')
        else:
            for stat in ['min', 'max', 'mean']:
                df_mv_cohort_final[f'{stat}_{vital_name.lower()}'] = np.nan
else:
    print("   âš ï¸ í™œë ¥ì§•í›„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

# ==============================================================================
# 4. ë°ì´í„° ì •ë¦¬ ë° ì €ì¥
# ==============================================================================
print("\n--- 4. ìµœì¢… ë°ì´í„° ì •ë¦¬ ë° ì €ì¥ ---")

# ë©”íƒ€ë°ì´í„° ë° ë¶ˆí•„ìš” ì»¬ëŸ¼ ì œê±° (ëª¨ë¸ ì…ë ¥ ë³€ìˆ˜ë§Œ ë‚¨ê¸°ê¸° ìœ„í•´)
# ìœ ì§€í•´ì•¼ í•  ì‹ë³„ì ë° íƒ€ê²Ÿ: 'HADM_ID' (ë‚˜ì¤‘ì— ì œê±°), 'hospital_mortality', 'ETHNICITY', 'INSURANCE'
meta_cols = [
    'ROW_ID', 'ICUSTAY_ID', 'HADM_ID',
    'INTIME', 'OUTTIME', 'ADMITTIME', 'DOB', 'DOD', 
    'ADMIT_YEAR', 'BIRTH_YEAR', 'ROW_ID_x', 'ROW_ID_y',
    'GENDER_x', 'GENDER_y', 'DOB_x', 'DOB_y',
    'FIRST_CAREUNIT', 'LAST_CAREUNIT', 'FIRST_WARDID', 'LAST_WARDID',
    'LOS', 'DBSOURCE', 'HOSPITAL_EXPIRE_FLAG'
]
# ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì œê±° (SUBJECT_IDëŠ” ìœ ì§€)
existing_cols_to_drop = [c for c in meta_cols if c in df_mv_cohort_final.columns]
df_features = df_mv_cohort_final.drop(columns=existing_cols_to_drop)

# GENDERë¥¼ ìˆ«ìë¡œ ë³€í™˜ (M=1, F=0)
if 'GENDER' in df_features.columns:
    df_features['GENDER'] = df_features['GENDER'].map({'M': 1, 'F': 0})

# íƒ€ê²Ÿ ë° ë¯¼ê° ë³€ìˆ˜ ì •ì˜
target_col = 'hospital_mortality'
sensitive_cols = ['ETHNICITY', 'INSURANCE']

# ëª¨ë¸ ì…ë ¥ ë³€ìˆ˜ ë¦¬ìŠ¤íŠ¸ (íƒ€ê²Ÿ, ë¯¼ê° ë³€ìˆ˜, ì‹ë³„ì ì œì™¸)
model_input_features = [c for c in df_features.columns if c not in [target_col, 'SUBJECT_ID'] + sensitive_cols]
print(f"âœ… ìµœì¢… ëª¨ë¸ ì…ë ¥ ë³€ìˆ˜ ê°œìˆ˜: {len(model_input_features)}")
print(f"   ë³€ìˆ˜ ëª©ë¡ (67ê°œ ì˜ˆìƒ): {model_input_features}")

if len(model_input_features) != 67:
    print(f"   âš ï¸ ê²½ê³ : ë³€ìˆ˜ ê°œìˆ˜ê°€ 67ê°œê°€ ì•„ë‹™ë‹ˆë‹¤! ({len(model_input_features)}ê°œ)")

# ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (Mean Imputation)
print("-> ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (Mean Imputation)...")
missing_threshold = 0.3
cols_to_drop = []
force_include_patterns = ['lactate', 'glucose']

for col in model_input_features:
    if any(pattern in col.lower() for pattern in force_include_patterns):
        continue
    if df_features[col].isnull().sum() / len(df_features) > missing_threshold:
        cols_to_drop.append(col)

if cols_to_drop:
    print(f"   ì œê±°ë  ë³€ìˆ˜ (Missing > {missing_threshold*100}%): {cols_to_drop}")
    df_features.drop(columns=cols_to_drop, inplace=True)
    # ì œê±° í›„ ë³€ìˆ˜ ëª©ë¡ ê°±ì‹ 
    model_input_features = [c for c in df_features.columns if c not in [target_col] + sensitive_cols]

# í‰ê· ê°’ ë³´ê°„
for col in model_input_features:
    if df_features[col].isnull().sum() > 0:
        mean_val = df_features[col].mean()
        df_features[col].fillna(mean_val, inplace=True)

# ë°ì´í„° ë¶„í•  ë° ì €ì¥
# ë°ì´í„° ì €ì¥ (ì „ì²´ ë°ì´í„°)
save_dir = DATA_PATH
save_dir.mkdir(exist_ok=True, parents=True)

# ì €ì¥ (ë¯¼ê° ë³€ìˆ˜ ë° íƒ€ê²Ÿ ì œì™¸í•œ X)
# SUBJECT_IDëŠ” ê° íŒŒì¼ì— ëª¨ë‘ í¬í•¨
X_all = df_features.drop(columns=sensitive_cols + [target_col]) # SUBJECT_ID í¬í•¨ë¨
y_all = df_features[[target_col, 'SUBJECT_ID']]
A_all = df_features[sensitive_cols + ['SUBJECT_ID']]

X_all.to_csv(save_dir / 'X_all.csv', index=False)
y_all.to_csv(save_dir / 'Y_all.csv', index=False)
A_all.to_csv(save_dir / 'A_all.csv', index=False)

print(f"   âœ… Saved all data to {save_dir}")

print("\nâœ¨ ëª¨ë“  ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ!")
